{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 20\n",
    "\n",
    "Hyperparameters\n",
    "\n",
    "NLP with Deep Learning DNN\n",
    "\n",
    "\n",
    "Its important where to start, \n",
    "For convecx loss function this does not matter.\n",
    "\n",
    "in NN loss functions nearly always are not convex loss functions so this does matter.\n",
    "\n",
    "\n",
    "Weight & (Step Size/Learning rate) are important\n",
    "\n",
    "Batch Normalization ->\n",
    "Normalise the batch after every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6036/1448965330.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2904\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m             \u001b[1;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "tensor_x = torch.Tensor(X)\n",
    "tensor_y = torch.Tensor(y)\n",
    "Xy = TensorDataset(tensor_x,tensor_y)\n",
    "Xy_loader = DataLoader(Xy,batch_size=16,shuffle=True,drop_last=True)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(inputDim,200),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(num_features=200),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(200,100),\n",
    "    nn.Tanh(),\n",
    "    nn.BatchNorm1d(num_features=100),\n",
    "    nn.Linear(100,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "nepochs = 100\n",
    "for epoch in range(nepochs):\n",
    "    for X,y in Xy_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(float(loss))\n",
    "with torch.no_grad():\n",
    "    xt = torch.tensor(np.random.rand(1,inputDim))\n",
    "    y2 = model(xt.float())\n",
    "    print(y2.detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.optim.Adam is one of the better optimizer\n",
    "\n",
    "\n",
    "Dropout is a way of inforcing the generalization and reduces the over fitting\n",
    "over fitting is a problem when you have a lot of pramaters to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing dropout\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "tensor_x = torch.Tensor(X)\n",
    "tensor_y = torch.Tensor(y)\n",
    "Xy = TensorDataset(tensor_x,tensor_y)\n",
    "Xy_loader = DataLoader(Xy,batch_size=16,shuffle=True,drop_last=True)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(inputDim,200),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(num_features=200),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(200,100),\n",
    "    nn.Tanh(),\n",
    "    nn.BatchNorm1d(num_features=100),\n",
    "    nn.Linear(100,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "nepochs = 100\n",
    "for epoch in range(nepochs):\n",
    "    for X,y in Xy_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(float(loss))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    xt = torch.tensor(np.random.rand(1,inputDim))\n",
    "    y2 = model(xt.float())\n",
    "    print(y2.detach().numpy()[0][0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping\n",
    "\n",
    "when training loss is decresing but the Validation loss increase at this point you might want to think about stopping\n",
    "\n",
    "Hyperparameters\n",
    "\n",
    "how do pick the number of layers\n",
    "\n",
    "how do you the number of units\n",
    "\n",
    "Activation function\n",
    "\n",
    "Init Learning rate\n",
    "\n",
    "Batch Size\n",
    "\n",
    "Dropout ratio\n",
    "\n",
    "Weight init\n",
    "\n",
    "Learning Rate decay policy\n",
    "\n",
    "Early Stopping \n",
    "\n",
    "\n",
    "No fixed way, to determine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /Data_cfar10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:33, 5033331.26it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Data_cfar10\\cifar-10-python.tar.gz to /Data_cfar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rootdir = '/Data_cfar10'\n",
    "T = datasets.CIFAR10(rootdir,train=True,download=True)\n",
    "V = datasets.CIFAR10(rootdir,train=False,download=True)\n",
    "\n",
    "len(T)\n",
    "len(V)\n",
    "type(T)\n",
    "\n",
    "X,y = T[12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x27E49A78F40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape\n",
    "32*32*3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "T = datasets.CIFAR10(rootdir,train=True,download=True,\n",
    "                    transform=transforms.ToTensor())\n",
    "V = datasets.CIFAR10(rootdir,train=False,download=True,\n",
    "                    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tL = torch.utils.data.DataLoader(T,batch_size=64,shuffle=True,drop_last=True)\n",
    "vL = torch.utils.data.DataLoader(V,batch_size=64,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(100,10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                      len(tL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1838.425048828125\n",
      "1799.234619140625\n"
     ]
    }
   ],
   "source": [
    "nepochs = 2\n",
    "for e in range(nepochs):\n",
    "    eLoss = 0\n",
    "    for X,y in tL:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        eLoss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(float(eLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10006009615384616\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "t = 0\n",
    "with torch.no_grad():\n",
    "    for Xv,yv in vL:\n",
    "        batch_size = Xv.shape[0]\n",
    "        y_hat = model(Xv.view(batch_size,-1))\n",
    "        _,p = torch.max(y_hat,dim=1)\n",
    "        t+=yv.shape[0]\n",
    "        c+=int((p==yv).sum())\n",
    "print(c/t)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
