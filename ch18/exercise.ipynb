{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(1000,)\n",
      "tensor(1.5915, dtype=torch.float64, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "\n",
    "def wSum(X,W):\n",
    "    h = torch.from_numpy(X)\n",
    "    z = torch.matmul(W,h)\n",
    "    return z\n",
    "\n",
    "def forwardStep(X,W_list):\n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h)\n",
    "        h = activate(z)\n",
    "    return h\n",
    "\n",
    "def activate(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "np.unique(y)\n",
    "\n",
    "W = torch.tensor(np.random.uniform(0,1,inputDim),requires_grad=True)\n",
    "\n",
    "z = wSum(X[0,:],W)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6984, dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True) #(2,inputDim) = (no. neurons, no imputs)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "z1 = forwardStep(X[0,:],W_list)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "completed up till dnn-forwardstep-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.4405]) tensor([14.4405])\n"
     ]
    }
   ],
   "source": [
    "#activation_fun = nn.Sigmoid()\n",
    "activation_fun = nn.ReLU()\n",
    "x = 100*torch.randn(1)\n",
    "print(x,activation_fun(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss functions\n",
    "\n",
    "f(xi) -> yi\n",
    "\n",
    "f is defined by the nerions in each layer of the Neural network\n",
    "\n",
    "The loss functions \n",
    "define loss finction\n",
    "random init W\n",
    "NN perfomrs produce targets\n",
    "then adjust the W \n",
    "and repeat to get a smaller loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4733240008354187\n",
      "tensor([0.6229]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "activation_fun = nn.Sigmoid()\n",
    "#activation_fun = nn.ReLU()\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float)\n",
    "y_hat = activation_fun(x)\n",
    "loss_fun = nn.BCELoss()\n",
    "\n",
    "loss_value = loss_fun(y_hat,y)\n",
    "\n",
    "print(loss_value.item())\n",
    "print(y_hat,y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
