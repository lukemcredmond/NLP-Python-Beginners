{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 12\n",
    "\n",
    "Markov assumption: n-gram\n",
    "\n",
    "Chain Rule\n",
    "\n",
    "P(how old are you)\n",
    "\n",
    "= p(how)*p(old|how)*p(are|how,old) * p*(you|how,old,are)\n",
    "\n",
    "this can be very slow if you have long sentances\n",
    "\n",
    "k=1 (bi-gram)\n",
    "\n",
    "\n",
    "p(are|how,old) = p(are|old)\n",
    "\n",
    "k=2 (tri-gram)\n",
    "\n",
    "p*(you|how,old,are) = p*(you|old,are)\n",
    "\n",
    "k-1 (k-gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def nGrams(doc_string_or_list,n,docIsAlreadyTokenized):\n",
    "    doc = doc_string_or_list\n",
    "    if not docIsAlreadyTokenized:\n",
    "        doc = [token.text for token in nlp(doc_string_or_list)]\n",
    "    doc = ' '.join(doc).lower().split(' ')\n",
    "    grams = [doc[i:i+n] for i in range(len(doc)-n+1)]\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how', 'old', 'are', 'you'], ['old', 'are', 'you', 'today'], ['are', 'you', 'today', 'or'], ['you', 'today', 'or', 'can'], ['today', 'or', 'can', 'you'], ['or', 'can', 'you', 'tell'], ['can', 'you', 'tell', 'me'], ['you', 'tell', 'me', 'something'], ['tell', 'me', 'something', 'about'], ['me', 'something', 'about', 'you']]\n"
     ]
    }
   ],
   "source": [
    "doc = 'how old are you today or can you tell me something about you'\n",
    "n = 4\n",
    "grams = nGrams(doc,n,False)\n",
    "print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def buildModel():\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    return model\n",
    "\n",
    "def updateCount(nGram,model):\n",
    "    w_1_to_n_minus_1 = tuple(nGram[:-1])\n",
    "    w_n = nGram[-1]\n",
    "    model[w_1_to_n_minus_1][w_n]+=1\n",
    "    return model\n",
    "\n",
    "def computeProbability(model):\n",
    "    for w_1_to_n_minus_1 in model:\n",
    "        totalCount = float(sum(model[w_1_to_n_minus_1].values()))\n",
    "        for w_n in model[w_1_to_n_minus_1]:\n",
    "            model[w_1_to_n_minus_1][w_n] /= totalCount\n",
    "    return model\n",
    "\n",
    "import dill\n",
    "\n",
    "def saveModel(model,fileNameDotpkl):\n",
    "    with open(fileNameDotpkl,'wb') as f:\n",
    "        dill.dump(model,f)\n",
    "\n",
    "def loadModel(fileNameDotpkl):\n",
    "    with open(fileNameDotpkl,'rb') as f:\n",
    "        model = dill.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups as getData\n",
    "from nltk.corpus import reuters as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "X,y = getData(subset='train',remove=('headers','footers','qoutes'),return_X_y=True)\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\lukem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lukem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "len(X)\n",
    "n = 3\n",
    "model = buildModel()\n",
    "\n",
    "for doc in X:\n",
    "    for nGram in nGrams(doc,n,False):\n",
    "        model = updateCount(nGram,model)\n",
    "\n",
    "\n",
    "for sentence in corpus.sents():\n",
    "    for nGram in nGrams(sentence,n,True):\n",
    "        model = updateCount(nGram,model)\n",
    "\n",
    "model = computeProbability(model)\n",
    "\n",
    "saveModel(model,'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "due to continued expansion of eep wheat to eight billion marks liquidity , bills maturing from may to boost latin output latin american \n",
      "\t baseball players _ ; it was ordered dismantled by the fall compares with today ' s subsidy bill .\n"
     ]
    }
   ],
   "source": [
    "text = ['after','that']\n",
    "nextWords = list(model[tuple(text[-n+1:])].keys())\n",
    "probs = list(model[tuple(text[-n+1:])].values())\n",
    "nextWord = np.random.choice(nextWords,1,probs)[0]\n",
    "print(nextWord)\n",
    "\n",
    "def sampleText(model,startingText=['after','that'],mxLenght = 100,nGramSize=3):\n",
    "    text = startingText\n",
    "    n = nGramSize\n",
    "    while not len(text)>mxLenght:\n",
    "        nextWords = list(model[tuple(text[-n+1:])].keys())\n",
    "        probs = list(model[tuple(text[-n+1:])].values())\n",
    "        if len(nextWords) > 0:\n",
    "            nextWord = np.random.choice(nextWords,1,probs)[0]\n",
    "            text.append(nextWord)\n",
    "        else:\n",
    "            break\n",
    "    sampled = ' '.join(text)\n",
    "    return sampled\n",
    "\n",
    "for sents in nlp(sampleText(model,['due','to'])).sents:\n",
    "    print(sents)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
