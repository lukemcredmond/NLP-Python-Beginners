{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 26\n",
    "\n",
    "Translation\n",
    "\n",
    "The final chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS\n",
      "EOS\n",
      "NLP\n",
      "How\n",
      "are\n",
      "you\n",
      "today\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.W2I = {'SOS':SOS_token,'EOS':EOS_token}\n",
    "        self.I2W = {SOS_token:'SOS',EOS_token:'EOS'}\n",
    "        self.W2C = {}\n",
    "        self.n_words = 2\n",
    "    def addSentence(self,s):\n",
    "        for word in s.split(' '):\n",
    "            self.addWord(word)\n",
    "    def addWord(self,w):\n",
    "        if w not in self.W2I:\n",
    "            self.W2I[w] = self.n_words\n",
    "            self.W2C[w] = 1\n",
    "            self.I2W[self.n_words] = w\n",
    "            self.n_words+=1\n",
    "        else:\n",
    "            self.W2C[w]+=1\n",
    "    def printAllWords(self):\n",
    "        words = list(self.W2I.keys())\n",
    "        for word in words:\n",
    "            print(word)\n",
    "\n",
    "L = Lang('Eng')\n",
    "L.addWord('NLP')\n",
    "L.addSentence('How are you today')\n",
    "L.printAllWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asfdieojo \n",
      "59 20753 29481\n",
      "['the young guy wants to drink.', 'le jeune homme veut boire.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python38\\lib\\site-packages\\torch\\cuda\\__init__.py:120: UserWarning: \n",
      "    Found GPU%d %s which is of cuda capability %d.%d.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is %d.%d.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.267714182535807\n",
      "10.299850781758625\n",
      "10.299460559421115\n",
      "10.313449827829997\n",
      "10.316380938575382\n",
      "10.310496006314716\n",
      "10.310403727836349\n",
      "10.308142578034174\n",
      "10.303793020097036\n",
      "10.30086672101702\n",
      "10.298527592910832\n",
      "10.298486091977074\n",
      "10.29776371547154\n",
      "10.293174478959063\n",
      "10.29390272231329\n",
      "10.289335240636552\n",
      "10.285893972915092\n",
      "10.287917304669737\n",
      "10.285961242905234\n",
      "10.287569214957099\n",
      "10.28461181744426\n",
      "10.28210162373332\n",
      "10.28239305863469\n",
      "10.28013473465329\n",
      "10.280115512666246\n",
      "10.2805825455867\n",
      "10.281644534111582\n",
      "10.28085302874074\n",
      "10.279765979931792\n",
      "10.281262133058416\n",
      "10.2803134407438\n",
      "10.278222553975997\n",
      "10.275736368591248\n",
      "10.27371424604642\n",
      "10.272897722279255\n",
      "10.27180066120936\n",
      "10.271480692242372\n",
      "10.270016587882376\n",
      "10.270371762762396\n",
      "10.17915883143399\n",
      "10.180819356008932\n",
      "10.183444075786575\n",
      "10.185545972755417\n",
      "10.188252231978721\n",
      "10.191026136304432\n",
      "10.19301643104728\n",
      "10.19477826526894\n",
      "10.195190627136846\n",
      "10.170095403592459\n",
      "10.124903592613459\n",
      "10.126893239421705\n",
      "10.080827144043493\n",
      "10.083099855785132\n",
      "10.08607444267396\n",
      "10.088891701362492\n",
      "10.0919312745819\n",
      "10.073172853333581\n",
      "10.051318342098433\n",
      "9.955613452285572\n",
      "9.892843719312095\n",
      "9.896367866473652\n",
      "9.902667687774228\n",
      "9.907939291494118\n",
      "9.873880253473184\n",
      "9.795827458933573\n",
      "9.781045954936143\n",
      "9.78774056501393\n",
      "9.793898638513554\n",
      "9.77565312957169\n",
      "9.782650080787871\n",
      "9.758323468551128\n",
      "9.766003234004826\n",
      "9.772544883194868\n",
      "9.736950911846533\n",
      "9.69346008187852\n",
      "9.69968717865177\n",
      "9.706198664681715\n",
      "9.713497786082316\n",
      "9.706124429778427\n",
      "9.644389582222686\n",
      "9.65039801003608\n",
      "9.603239771744088\n",
      "9.55977471185591\n",
      "9.568120289365439\n",
      "9.575859207215016\n",
      "9.583619320065464\n",
      "9.54739715034111\n",
      "9.554053811002849\n",
      "9.561989757939658\n",
      "9.569427654960409\n",
      "9.576834568134661\n",
      "9.570259741644131\n",
      "9.556011782970117\n",
      "9.562312122655317\n",
      "9.569201502762406\n",
      "9.575989931701848\n",
      "9.520142932655126\n",
      "9.470632929321699\n",
      "9.457991700804836\n",
      "9.466066555601598\n"
     ]
    }
   ],
   "source": [
    "def unicode2ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD',s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicode2ascii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])',r'\\1',s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+',r' ',s)\n",
    "    return s\n",
    "\n",
    "print(normalizeString('asfdieojo98793259'))\n",
    "\n",
    "#read file\n",
    "def readLangs():\n",
    "    lines = open('../data/eng-fra.txt',encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    input_lang = Lang('eng')\n",
    "    output_lang = Lang('fra')\n",
    "    return input_lang, output_lang,pairs\n",
    "\n",
    "#read in file and build pairs\n",
    "I,O,P = readLangs()\n",
    "\n",
    "\n",
    "#video26_5\n",
    "\n",
    "def prepareData(I,O,P):\n",
    "    MAX_LENGTH = 0\n",
    "    for pair in P:\n",
    "        I.addSentence(pair[0])\n",
    "        O.addSentence(pair[1])\n",
    "        MAX_LENGTH = max(MAX_LENGTH,len(pair[0].split()),len(pair[1].split()))\n",
    "    return I,O,MAX_LENGTH\n",
    "\n",
    "input_lang,output_lang,MAX_LENGTH = prepareData(I,O,P)\n",
    "\n",
    "print(MAX_LENGTH,input_lang.n_words,output_lang.n_words)\n",
    "\n",
    "#output_lang.printAllWords()\n",
    "print(random.choice(P))\n",
    "pairs = P\n",
    "\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,vocabSize,hidden_size):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.E = nn.Embedding(vocabSize,hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,\n",
    "                          batch_first=True,bidirectional=True)\n",
    "    def forward(self,input,hidden):\n",
    "        emb = self.E(input).view(1,1,-1)\n",
    "        output,hidden = self.gru(emb,hidden)\n",
    "        return output,hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2,1,self.hidden_size,device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,vocabSize,max_length = MAX_LENGTH):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = vocabSize\n",
    "        self.max_length = max_length\n",
    "        self.E = nn.Embedding(self.output_size,self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2,self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*3,self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size,self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size,self.output_size)\n",
    "    \n",
    "    def forward(self,input,hidden,encoder_outputs):\n",
    "        emb = self.E(input).view(1,1,-1)\n",
    "        attn_w = F.softmax(self.attn(torch.cat((emb[0],hidden[0]),1)),dim=1)\n",
    "        attn_A = torch.bmm(attn_w.unsqueeze(0),\n",
    "                          encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((emb[0],attn_A[0]),1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output,hidden = self.gru(output,hidden)\n",
    "        output = F.log_softmax(self.out(output[0]),dim=1)\n",
    "        return output,hidden,attn_w\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size,device=device)\n",
    "\n",
    "def indexesFromSentence(lang,s):\n",
    "    return[lang.W2I[w] for w in s.split()]\n",
    "\n",
    "def tensorFromSentence(lang,s):\n",
    "    idx = indexesFromSentence(lang,s)\n",
    "    idx.append(EOS_token)\n",
    "    return torch.tensor(idx,dtype=torch.long,device=device).view(-1,1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang,pair[0])\n",
    "    output_tensor = tensorFromSentence(output_lang,pair[1])\n",
    "    return (input_tensor,output_tensor)\n",
    "\n",
    "def train(input_tensor,target_tensor,encoder,decoder,\n",
    "         encoder_optimizer,decoder_optimizer,loss_fn,\n",
    "         max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length= target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length,2*encoder.hidden_size,device=device)\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output,encoder_hidden = encoder(\n",
    "            input_tensor[ei],encoder_hidden)\n",
    "        out_reshaped = encoder_output.view(1,1,2,encoder.hidden_size)\n",
    "        out_fwd = out_reshaped[:,:,0,:]\n",
    "        out_bck = out_reshaped[:,:,1,:]\n",
    "        encoder_outputs[ei] = torch.cat((out_fwd[0,0],out_bck[0,0]),0)\n",
    "    decoder_input = torch.tensor([[SOS_token]],device=device)\n",
    "    h_reshaped = encoder_hidden.view(1,2,1,encoder.hidden_size)\n",
    "    decoder_hidden = h_reshaped[:,0,:,:]\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output,decoder_hidden,decoder_attention = decoder(\n",
    "            decoder_input,decoder_hidden,encoder_outputs\n",
    "        )\n",
    "        topv,topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss+=loss_fn(decoder_output,target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()/target_length\n",
    "\n",
    "def trainIters(encoder,decoder,n_iters,lr=0.001):\n",
    "    totalLoss = 0\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(),lr=lr)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(),lr=lr)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                     for i in range(n_iters)]\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    for iter in range(n_iters):\n",
    "        training_pair = training_pairs[iter]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = train(input_tensor,target_tensor,encoder,decoder,\n",
    "                        encoder_optimizer,decoder_optimizer,loss_fn\n",
    "                    )\n",
    "        totalLoss+=loss\n",
    "        print(totalLoss/(iter+1))\n",
    "    \n",
    "hidden_size = 128\n",
    "encoder = EncoderRNN(input_lang.n_words,hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size,output_lang.n_words).to(device)\n",
    "trainIters(encoder,decoder,100)\n",
    "\n",
    "def evaluate(encoder,decoder,s,max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang,s)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length,2*encoder.hidden_size,device=device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output,encoder_hidden = encoder(\n",
    "                input_tensor[ei],encoder_hidden)\n",
    "            out_reshaped = encoder_output.view(1,1,2,encoder.hidden_size)\n",
    "            out_fwd = out_reshaped[:,:,0,:]\n",
    "            out_bck = out_reshaped[:,:,1,:]\n",
    "            encoder_outputs[ei] = torch.cat((out_fwd[0,0],out_bck[0,0]),0)\n",
    "        decoder_input = torch.tensor([[SOS_token]],device=device)\n",
    "        h_reshaped = encoder_hidden.view(1,2,1,encoder.hidden_size)\n",
    "        decoder_hidden = h_reshaped[:,0,:,:]\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_att = torch.zeros(max_length,max_length)\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output,decoder_hidden,decoder_attention = decoder(\n",
    "                decoder_input,decoder_hidden,encoder_outputs\n",
    "            )\n",
    "            decoder_att[di] = decoder_attention.data\n",
    "            topv,topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.I2W[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meritez', 'surfez', 'comprendrais.', 'ramenerai', 'singapour.', '<EOS>'] va !\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(encoder,decoder,pairs[0][0]),pairs[0][1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
