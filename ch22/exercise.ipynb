{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 22\n",
    "\n",
    "Language Modeling toy example\n",
    "\n",
    "i like this black cat.\n",
    "\n",
    "prodict next word\n",
    "\n",
    "i like this [] <= black\n",
    "\n",
    "\n",
    "vocab: \"100 words\"\n",
    "\n",
    "vocab\n",
    "word -> index\n",
    "i -> 2\n",
    "like -> 45\n",
    "this -> 30\n",
    "black -> 55\n",
    "cat -> 10\n",
    ". -> 1\n",
    "\n",
    "our goal is to prodict \n",
    "\"i like this black cat []\" <= .\n",
    "\n",
    "input => i like this black cat\n",
    "target => like this black cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "\n",
    "inputString = [2,45,30,55,10]\n",
    "outputString = [45,30,55,10,1]\n",
    "\n",
    "numFeatures = 100\n",
    "vocabSize = 80\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "for i in range(len(inputString)):\n",
    "    x = np.random.randn(numFeatures,1)\n",
    "    embeddings.append(x)\n",
    "\n",
    "\n",
    "embeddings[0].shape\n",
    "len(embeddings)\n",
    "\n",
    "def getOneHot(idx):\n",
    "    one_hot = np.zeros((vocabSize,1))\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "print(getOneHot(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 50]) torch.Size([50, 100]) torch.Size([80, 50]) torch.Size([50, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numUnits = 50\n",
    "h0 = torch.tensor(np.zeros((numUnits,1)))\n",
    "Wh = torch.tensor(np.random.uniform(0,1,(numUnits,numUnits)),requires_grad=True)\n",
    "Wx = torch.tensor(np.random.uniform(0,1,(numUnits,numFeatures)),requires_grad=True)\n",
    "Wy = torch.tensor(np.random.uniform(0,1,(vocabSize,numUnits)),requires_grad=True)\n",
    "\n",
    "print(Wh.shape,Wx.shape,Wy.shape,h0.shape)\n",
    "\n",
    "def stepForward(xt,Wx,Wh,Wy,prevMemory):\n",
    "    x_frd = torch.matmul(Wx,torch.from_numpy(xt))\n",
    "    h_frd = torch.matmul(Wh,prevMemory)\n",
    "    ht = torch.tanh(x_frd+h_frd)\n",
    "    yt_hat = F.softmax(torch.matmul(Wy,ht),dim=0)\n",
    "    return ht,yt_hat\n",
    "\n",
    "ht,yt_hat = stepForward(embeddings[0],Wx,Wh,Wy,h0)\n",
    "ht.shape\n",
    "yt_hat.shape\n",
    "yt_hat.sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullForwardRNN(X,Wx,Wh,Wy,prevMemory):\n",
    "    y_hat = []\n",
    "    for t in range(len(X)):\n",
    "        ht,yt_hat = stepForward(X[t],Wx,Wh,Wy,prevMemory)\n",
    "        prevMemory = ht\n",
    "        y_hat.append(yt_hat)\n",
    "    return y_hat  \n",
    "\n",
    "y_hat = fullForwardRNN(embeddings,Wx,Wh,Wy,h0)\n",
    "\n",
    "len(y_hat)\n",
    "y_hat[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.9592], dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Loss after epoch=0: 7.959230\n",
      "Loss after epoch=1: 7.933272\n",
      "Loss after epoch=2: 7.907358\n",
      "Loss after epoch=3: 7.881489\n",
      "Loss after epoch=4: 7.855665\n",
      "Loss after epoch=5: 7.829885\n",
      "Loss after epoch=6: 7.804150\n",
      "Loss after epoch=7: 7.778461\n",
      "Loss after epoch=8: 7.752816\n",
      "Loss after epoch=9: 7.727217\n",
      "Loss after epoch=10: 7.701663\n",
      "Loss after epoch=11: 7.676154\n",
      "Loss after epoch=12: 7.650692\n",
      "Loss after epoch=13: 7.625276\n",
      "Loss after epoch=14: 7.599906\n",
      "Loss after epoch=15: 7.574583\n",
      "Loss after epoch=16: 7.549308\n",
      "Loss after epoch=17: 7.524079\n",
      "Loss after epoch=18: 7.498899\n",
      "Loss after epoch=19: 7.473767\n",
      "Loss after epoch=20: 7.448683\n",
      "Loss after epoch=21: 7.423649\n",
      "Loss after epoch=22: 7.398664\n",
      "Loss after epoch=23: 7.373730\n",
      "Loss after epoch=24: 7.348846\n",
      "Loss after epoch=25: 7.324013\n",
      "Loss after epoch=26: 7.299232\n",
      "Loss after epoch=27: 7.274504\n",
      "Loss after epoch=28: 7.249828\n",
      "Loss after epoch=29: 7.225207\n",
      "Loss after epoch=30: 7.200639\n",
      "Loss after epoch=31: 7.176126\n",
      "Loss after epoch=32: 7.151669\n",
      "Loss after epoch=33: 7.127269\n",
      "Loss after epoch=34: 7.102925\n",
      "Loss after epoch=35: 7.078639\n",
      "Loss after epoch=36: 7.054411\n",
      "Loss after epoch=37: 7.030242\n",
      "Loss after epoch=38: 7.006134\n",
      "Loss after epoch=39: 6.982085\n",
      "Loss after epoch=40: 6.958099\n",
      "Loss after epoch=41: 6.934174\n",
      "Loss after epoch=42: 6.910311\n",
      "Loss after epoch=43: 6.886512\n",
      "Loss after epoch=44: 6.862778\n",
      "Loss after epoch=45: 6.839108\n",
      "Loss after epoch=46: 6.815504\n",
      "Loss after epoch=47: 6.791966\n",
      "Loss after epoch=48: 6.768494\n",
      "Loss after epoch=49: 6.745091\n",
      "Loss after epoch=50: 6.721755\n",
      "Loss after epoch=51: 6.698489\n",
      "Loss after epoch=52: 6.675292\n",
      "Loss after epoch=53: 6.652165\n",
      "Loss after epoch=54: 6.629108\n",
      "Loss after epoch=55: 6.606123\n",
      "Loss after epoch=56: 6.583210\n",
      "Loss after epoch=57: 6.560368\n",
      "Loss after epoch=58: 6.537600\n",
      "Loss after epoch=59: 6.514904\n",
      "Loss after epoch=60: 6.492283\n",
      "Loss after epoch=61: 6.469735\n",
      "Loss after epoch=62: 6.447262\n",
      "Loss after epoch=63: 6.424864\n",
      "Loss after epoch=64: 6.402541\n",
      "Loss after epoch=65: 6.380294\n",
      "Loss after epoch=66: 6.358122\n",
      "Loss after epoch=67: 6.336027\n",
      "Loss after epoch=68: 6.314008\n",
      "Loss after epoch=69: 6.292066\n",
      "Loss after epoch=70: 6.270201\n",
      "Loss after epoch=71: 6.248412\n",
      "Loss after epoch=72: 6.226702\n",
      "Loss after epoch=73: 6.205068\n",
      "Loss after epoch=74: 6.183512\n",
      "Loss after epoch=75: 6.162034\n",
      "Loss after epoch=76: 6.140634\n",
      "Loss after epoch=77: 6.119312\n",
      "Loss after epoch=78: 6.098067\n",
      "Loss after epoch=79: 6.076901\n",
      "Loss after epoch=80: 6.055813\n",
      "Loss after epoch=81: 6.034802\n",
      "Loss after epoch=82: 6.013870\n",
      "Loss after epoch=83: 5.993016\n",
      "Loss after epoch=84: 5.972239\n",
      "Loss after epoch=85: 5.951541\n",
      "Loss after epoch=86: 5.930920\n",
      "Loss after epoch=87: 5.910377\n",
      "Loss after epoch=88: 5.889912\n",
      "Loss after epoch=89: 5.869525\n",
      "Loss after epoch=90: 5.849215\n",
      "Loss after epoch=91: 5.828982\n",
      "Loss after epoch=92: 5.808827\n",
      "Loss after epoch=93: 5.788748\n",
      "Loss after epoch=94: 5.768747\n",
      "Loss after epoch=95: 5.748822\n",
      "Loss after epoch=96: 5.728974\n",
      "Loss after epoch=97: 5.709203\n",
      "Loss after epoch=98: 5.689508\n",
      "Loss after epoch=99: 5.669888\n"
     ]
    }
   ],
   "source": [
    "def computeLoss(y,y_hat):#y => 1 hope vectors , y_hat => prodictions of model\n",
    "    loss = 0\n",
    "    for yi,yi_hat in zip(y,y_hat):\n",
    "        Li = -torch.log2(yi_hat[yi==1])\n",
    "        loss += Li\n",
    "    return loss/len(y)\n",
    "\n",
    "y = []\n",
    "for idx in outputString:\n",
    "    y.append(getOneHot(idx))\n",
    "\n",
    "print(computeLoss(y,y_hat))\n",
    "\n",
    "def updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr):\n",
    "    with torch.no_grad():\n",
    "        Wx -= lr*dWx\n",
    "        Wh -= lr*dWh\n",
    "        Wy -= lr*dWy\n",
    "    return Wx,Wh,Wy\n",
    "\n",
    "def trainRNN(X,y,Wx,Wh,Wy,prevMemory,lr,nepoch):\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        y_hat = fullForwardRNN(X,Wx,Wh,Wy,prevMemory)\n",
    "        loss = computeLoss(y,y_hat)\n",
    "        loss.backward()\n",
    "        losses.append(loss)\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "        sys.stdout.flush()\n",
    "        dWx = Wx.grad.data\n",
    "        dWh = Wh.grad.data\n",
    "        dWy = Wy.grad.data\n",
    "        Wx,Wh,Wy = updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr)\n",
    "        Wx.grad.data.zero_()\n",
    "        Wh.grad.data.zero_()\n",
    "        Wy.grad.data.zero_()\n",
    "    return Wx,Wh,Wy,losses\n",
    "\n",
    "Wx,Wh,Wy,losses = trainRNN(embeddings,y,Wx,Wh,Wy,h0,0.001,100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
