{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 22\n",
    "\n",
    "Language Modeling toy example\n",
    "\n",
    "i like this black cat.\n",
    "\n",
    "prodict next word\n",
    "\n",
    "i like this [] <= black\n",
    "\n",
    "\n",
    "vocab: \"100 words\"\n",
    "\n",
    "vocab\n",
    "word -> index\n",
    "i -> 2\n",
    "like -> 45\n",
    "this -> 30\n",
    "black -> 55\n",
    "cat -> 10\n",
    ". -> 1\n",
    "\n",
    "our goal is to prodict \n",
    "\"i like this black cat []\" <= .\n",
    "\n",
    "input => i like this black cat\n",
    "target => like this black cat.\n",
    "\n",
    "x(t){Wx}  ->[tanh]->[softmax]->y^(t)\n",
    "h(t-1){Wy}->[tanh]->h(t)\n",
    "\n",
    "\n",
    "h0 = numFeatures*1 = 100\n",
    "[tanh] = no. units = 50\n",
    "Wx = no. units * num Features = 50*100\n",
    "Wy = no. units * vocab size = 50*80\n",
    "Wh = no. units * history len = 50*50\n",
    "h(t) = 50 * 1\n",
    "\n",
    "we need to convert the input history into tanh form this can be done via "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "\n",
    "# function to get one hot encoding \n",
    "def getOneHot(idx):\n",
    "    one_hot = np.zeros((vocabSize,1))\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "def stepForward(xt,Wx,Wh,outputWeight_Wy,prevMemory):\n",
    "    x_frd = torch.matmul(Wx,torch.from_numpy(xt))\n",
    "    h_frd = torch.matmul(Wh,prevMemory)\n",
    "    ht = torch.tanh(x_frd+h_frd)\n",
    "    yt_hat = F.softmax(torch.matmul(outputWeight_Wy,ht),dim=0)\n",
    "    return ht,yt_hat\n",
    "\n",
    "def fullForwardRNN(X,Wx,Wh,outputWeight_Wy,prevMemory):\n",
    "    y_hat = []\n",
    "    for t in range(len(X)):\n",
    "        ht,yt_hat = stepForward(X[t],Wx,Wh,outputWeight_Wy,prevMemory)\n",
    "        prevMemory = ht\n",
    "        y_hat.append(yt_hat)\n",
    "    return y_hat  \n",
    "\n",
    "def computeLoss(y,y_hat):#y => 1 hope vectors , y_hat => prodictions of model\n",
    "    loss = 0\n",
    "    for yi,yi_hat in zip(y,y_hat):\n",
    "        Li = -torch.log2(yi_hat[yi==1])\n",
    "        loss += Li\n",
    "    return loss/len(y)\n",
    "\n",
    "def updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr):\n",
    "    with torch.no_grad():\n",
    "        Wx -= lr*dWx\n",
    "        Wh -= lr*dWh\n",
    "        Wy -= lr*dWy\n",
    "    return Wx,Wh,Wy\n",
    "\n",
    "def trainRNN(X,y,Wx,Wh,Wy,prevMemory,lr,nepoch):\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        y_hat = fullForwardRNN(X,Wx,Wh,Wy,prevMemory)\n",
    "        loss = computeLoss(y,y_hat)\n",
    "        loss.backward()\n",
    "        losses.append(loss)\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "        sys.stdout.flush()\n",
    "        dWx = Wx.grad.data\n",
    "        dWh = Wh.grad.data\n",
    "        dWy = Wy.grad.data\n",
    "        Wx,Wh,Wy = updateParams(Wx,Wh,Wy,dWx,dWh,dWy,lr)\n",
    "        Wx.grad.data.zero_()\n",
    "        Wh.grad.data.zero_()\n",
    "        Wy.grad.data.zero_()\n",
    "    return Wx,Wh,Wy,losses\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Magic Values and dont know where they came from\n",
    "# cannot find where he picked this number from\n",
    "#-------------------------------------------------\n",
    "# number of features\n",
    "numFeatures = 100\n",
    "# number of units in a Neural Network cell\n",
    "numUnits = 50\n",
    "\n",
    "\n",
    "\n",
    "# this is faked but we would load in a document of some kind\n",
    "# build a dictionary of every word and assign an index\n",
    "# given a sentence change the sentence from text to an array of indexes\n",
    "\n",
    "vocabSize = 80\n",
    "inputString = [2,45,30,55,10]\n",
    "outputString = [45,30,55,10,1]\n",
    "\n",
    "# each index in our vocab needs to have some embedding\n",
    "# an array of values that has some meaning\n",
    "# the meaning and how they are calculated was not covered\n",
    "# but this should be calculated when getting the vocab\n",
    "word_embeddings = []\n",
    "for i in range(len(inputString)):\n",
    "    x = np.random.randn(numFeatures,1)\n",
    "    word_embeddings.append(x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 50]) torch.Size([50, 100]) torch.Size([80, 50]) torch.Size([50, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "h0 = torch.tensor(np.zeros((numUnits,1)))\n",
    "historyWeight_Wh = torch.tensor(np.random.uniform(0,1,(numUnits,numUnits)),requires_grad=True)\n",
    "featureWeight_Wx = torch.tensor(np.random.uniform(0,1,(numUnits,numFeatures)),requires_grad=True)\n",
    "outputWeight_Wy = torch.tensor(np.random.uniform(0,1,(vocabSize,numUnits)),requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ht,yt_hat = stepForward(word_embeddings[0],featureWeight_Wx,historyWeight_Wh,outputWeight_Wy,h0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_hat = fullForwardRNN(word_embeddings,featureWeight_Wx,historyWeight_Wh,outputWeight_Wy,h0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = []\n",
    "for idx in outputString:\n",
    "    y.append(getOneHot(idx))\n",
    "\n",
    "#print(computeLoss(y,y_hat))\n",
    "\n",
    "\n",
    "\n",
    "Wx,Wh,Wy,losses = trainRNN(word_embeddings,y,featureWeight_Wx,historyWeight_Wh,outputWeight_Wy,h0,0.001,100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
