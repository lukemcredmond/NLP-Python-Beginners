{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 16\n",
    "\n",
    "Word2Vec\n",
    "\n",
    "Train classifer on a binary prediction task\n",
    "\n",
    "Self-supervision sample data taken from the corpus\n",
    "\n",
    "see if something is good or bad\n",
    "\n",
    "\n",
    "Negative Sampling\n",
    "p(Ci is context |W) 1 or 0\n",
    "\n",
    "Skip Gram Negative Sampling (SGNS)\n",
    "\n",
    "use a +/- 2 window\n",
    "\n",
    "need to create a negative pair at least k amount (k = 4)\n",
    "this can be done by taking the word and randomly selecting a word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups as getData\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "X,y = getData(subset='train',remove=('headers','footers','quotes'),return_X_y=True)\n",
    "\n",
    "print(X)\n",
    "print(X[0])\n",
    "\n",
    "#get all tokens and there frequence\n",
    "fr = defaultdict(int)\n",
    "for doc in X:\n",
    "    for token in re.split('\\W+',doc.lower()):\n",
    "        fr[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1328/2847284142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumWords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumWords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vecSGNS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocessed_corpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2I\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mI2W\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1328/2847284142.py\u001b[0m in \u001b[0;36mword2vecSGNS\u001b[1;34m(W, C, doc, vocab, W2I, I2W, probs, winSize, k)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mnegIdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mnegC\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                     \u001b[0mnegIdx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2I\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnegC\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[0mcNeg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mW2I\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnegC\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#any token appears less than 10 times remove\n",
    "cutOffValue = 10\n",
    "processed_corpus = []\n",
    "for doc in X:\n",
    "    for token in re.split('\\W+',doc.lower()):\n",
    "        if fr[token] >= cutOffValue:\n",
    "            processed_corpus.append(token)\n",
    "\n",
    "\n",
    "allWords = np.array(list(fr.keys()))#all words\n",
    "allCounts = np.array(list(fr.values()))#there counts\n",
    "vocab = allWords[allCounts >= cutOffValue]#all words having counts > 10\n",
    "wordCounts = allCounts[allCounts >= cutOffValue]#all counts > 10\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.75 #alpha for negative sampling\n",
    "wordCounts = wordCounts**alpha #help array to increase prop\n",
    "probs = wordCounts/np.sum(wordCounts)#all probs\n",
    "\n",
    "numWords = len(vocab)\n",
    "W2I = dict(zip(vocab,np.arange(numWords)))#build model of the word and count\n",
    "I2W = dict(zip(np.arange(numWords),vocab))#reverse of the above\n",
    "\n",
    "#\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#\n",
    "def gradientStep(w,cPos,cNeg,lr=0.001):\n",
    "    cPos_new = cPos - lr*(sigmoid(w.dot(cPos))-1)*w\n",
    "    w_new = w - lr*(sigmoid(w.dot(cPos))-1)*cPos\n",
    "    cNeg_new = np.zeros(cNeg.shape)\n",
    "    for i in range(cNeg.shape[0]):\n",
    "        v = sigmoid(cNeg[i,:].dot(w))\n",
    "        w_new -= lr*v*cNeg[i,:]\n",
    "        cNeg_new[i,:] = cNeg[i,:] - lr*v*w\n",
    "    return w_new,cPos_new,cNeg_new\n",
    "\n",
    "#W=init embedings\n",
    "#C=context embedings\n",
    "#doc=document in list form\n",
    "#vocab=vocabulary\n",
    "#W2I=word to index mapping\n",
    "#I2W=index to word mapping\n",
    "#probs= probabilites of vocabulary\n",
    "#winSize=window size\n",
    "#k=\n",
    "def word2vecSGNS(W,C,doc,vocab,W2I,I2W,probs,winSize=2,k=4):\n",
    "    numTokens = len(doc) #len of dcoument\n",
    "    for i in range(numTokens):\n",
    "        for j in range(i-winSize,i+winSize):#\n",
    "            if j != i and j >= 0 and j < numTokens:\n",
    "                wIdx = W2I[doc[i]]\n",
    "                posIdx = W2I[doc[j]]\n",
    "                w = W[wIdx,:]\n",
    "                cPos = C[posIdx,:]#this is one pair of many\n",
    "                cNeg = np.zeros((k,C.shape[1]))\n",
    "                m = 0\n",
    "                negIdx = []\n",
    "                for negC in np.random.choice(list(vocab),k,list(probs)):\n",
    "                    negIdx.append(W2I[negC])\n",
    "                    cNeg[m,:] = C[W2I[negC],:]\n",
    "                    m += 1\n",
    "        w_new,cPos_new,cNeg_new = gradientStep(w,cPos,cNeg)\n",
    "        W[wIdx,:] = w_new\n",
    "        C[posIdx,:] = cPos_new\n",
    "        C[negIdx,:] = cNeg_new\n",
    "    return W,C\n",
    "\n",
    "W = np.random.rand(numWords,100)\n",
    "C = np.random.rand(numWords,100)\n",
    "W,C = word2vecSGNS(W,C,processed_corpus,vocab,W2I,I2W,probs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
