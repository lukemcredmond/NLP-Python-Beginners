{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 24 RNN with PyTourch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "tensor([[ 6.8967e-01,  8.3772e-01, -1.4253e+00, -2.2678e+00,  1.5286e-01,\n",
      "         -3.6639e-01,  3.8182e-01, -2.4628e-01,  4.3394e-01,  1.3959e+00],\n",
      "        [ 4.4284e-01,  9.9146e-02, -1.7342e-03,  1.0122e+00,  5.2173e-01,\n",
      "         -1.8305e-01, -1.3160e+00, -2.2298e-01, -1.2828e-02,  6.2070e-01],\n",
      "        [ 2.5847e+00, -7.1213e-01,  3.7544e-01,  1.2159e+00, -5.1056e-02,\n",
      "          3.9151e-01, -1.0942e-01, -8.8341e-01, -4.6353e-01,  1.4345e-01]])\n",
      "tensor([[ 1.0976, -1.3319, -0.3476, -1.0160,  2.2616, -0.5990,  0.6283,  0.4848,\n",
      "          0.1851,  1.7402],\n",
      "        [-0.6835, -1.4256, -0.4018, -1.3473, -0.0106, -0.1029,  0.1179,  0.3500,\n",
      "         -0.7413,  0.6821],\n",
      "        [ 1.5700, -1.4144, -1.5505, -1.0328,  0.1556, -0.0759,  1.1951, -0.6270,\n",
      "          0.5250, -1.3029]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocabSize = 20\n",
    "featureSize = 10\n",
    "\n",
    "W = torch.randn(vocabSize,featureSize)#creates matrixs 20x10\n",
    "print(W.shape)\n",
    "\n",
    "E = nn.Embedding.from_pretrained(W)#this embedding makes lookups faster.\n",
    "\n",
    "idx = torch.LongTensor([1,2,5])\n",
    "data = E(idx)\n",
    "print(data)\n",
    "\n",
    "E = nn.Embedding(vocabSize,featureSize)#random embeddings not fixed\n",
    "data = E(idx)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 10])\n",
      "tensor([[[ 0.4542, -0.8756,  0.8605, -0.8056],\n",
      "         [ 0.6031, -0.8361, -0.4437,  0.5223],\n",
      "         [ 0.9453, -0.9016,  0.6979, -0.6130],\n",
      "         [ 0.8633, -0.6318, -0.0184,  0.4607],\n",
      "         [ 0.4501, -0.7344,  0.6388, -0.0063]],\n",
      "\n",
      "        [[ 0.3933, -0.2593, -0.6747,  0.8502],\n",
      "         [ 0.6916, -0.9222,  0.8964, -0.8075],\n",
      "         [ 0.6295, -0.2666, -0.2048,  0.6694],\n",
      "         [ 0.7778, -0.7592, -0.0585,  0.1553],\n",
      "         [ 0.5884, -0.8424,  0.7280, -0.2241]],\n",
      "\n",
      "        [[ 0.1505, -0.7063,  0.6789, -0.5759],\n",
      "         [ 0.7637, -0.5323, -0.7595,  0.8407],\n",
      "         [ 0.5063, -0.9427,  0.8496, -0.6219],\n",
      "         [ 0.7555, -0.9030,  0.5027, -0.2897],\n",
      "         [ 0.6095, -0.8222,  0.0912, -0.0686]],\n",
      "\n",
      "        [[-0.2271, -0.8925,  0.7370, -0.3857],\n",
      "         [ 0.2084, -0.9297,  0.5034,  0.1139],\n",
      "         [ 0.6838, -0.9451,  0.4152, -0.0731],\n",
      "         [ 0.1723, -0.8817,  0.8375, -0.1438],\n",
      "         [ 0.2235, -0.8790,  0.2395,  0.0550]]], grad_fn=<TransposeBackward1>) tensor([[[ 0.9749,  0.8607],\n",
      "         [ 0.9802,  0.7695],\n",
      "         [ 0.9803, -0.7708],\n",
      "         [ 0.9313,  0.8953]],\n",
      "\n",
      "        [[-0.2009, -0.8476],\n",
      "         [-0.8614,  0.9671],\n",
      "         [ 0.0533,  0.6681],\n",
      "         [ 0.9620, -0.3982]],\n",
      "\n",
      "        [[-0.5779, -0.0741],\n",
      "         [-0.4226,  0.2401],\n",
      "         [ 0.7855, -0.6817],\n",
      "         [ 0.0619, -0.7528]],\n",
      "\n",
      "        [[-0.3036,  0.2674],\n",
      "         [ 0.2020, -0.6521],\n",
      "         [-0.4376, -0.6040],\n",
      "         [-0.8532,  0.5269]],\n",
      "\n",
      "        [[ 0.4501, -0.7344],\n",
      "         [ 0.5884, -0.8424],\n",
      "         [ 0.6095, -0.8222],\n",
      "         [ 0.2235, -0.8790]],\n",
      "\n",
      "        [[ 0.8605, -0.8056],\n",
      "         [-0.6747,  0.8502],\n",
      "         [ 0.6789, -0.5759],\n",
      "         [ 0.7370, -0.3857]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([6, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "vocabSize = 20\n",
    "featureSize = 10\n",
    "\n",
    "sqLength = 5#batch data in length of input words 5\n",
    "hiddenSize = 2 # total number of nurions\n",
    "numLayers = 1 # how many stack layers\n",
    "batchSize = 4 # vocabSize / sqLength\n",
    "\n",
    "idx = torch.LongTensor(np.arange(vocabSize))\n",
    "E = nn.Embedding(vocabSize,featureSize)# 20 vectors and each vector will be 10 dimensional\n",
    "data = E(idx)\n",
    "\n",
    "inputs = data.view(batchSize,sqLength,featureSize)\n",
    "print(inputs.shape)\n",
    "\n",
    "rnn = nn.RNN(input_size=featureSize,hidden_size=hiddenSize,\n",
    "             num_layers=numLayers,batch_first=True)\n",
    "\n",
    "rnn_gru = nn.GRU(input_size=featureSize,hidden_size=hiddenSize,\n",
    "             num_layers=numLayers,batch_first=True)\n",
    "\n",
    "rnn_lstm = nn.LSTM(input_size=featureSize,hidden_size=hiddenSize,\n",
    "             num_layers=numLayers,batch_first=True)\n",
    "\n",
    "rnn_bi = nn.RNN(input_size=featureSize,hidden_size=hiddenSize,\n",
    "             num_layers=3,batch_first=True,bidirectional=True)\n",
    "\n",
    "#y_hat, h = rnn(inputs)\n",
    "#print(y_hat)\n",
    "y_hat, h = rnn_bi(inputs)\n",
    "print(y_hat, h)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7592, grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 5, 2])\n",
      "torch.Size([6, 4, 2])\n",
      "torch.Size([3, 4, 2])\n",
      "torch.Size([4, 5, 2])\n",
      "torch.Size([1, 4, 2]) torch.Size([1, 4, 2])\n",
      "torch.Size([4, 5, 2])\n",
      "tensor(0.9235, grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 4, 2])\n",
      "tensor([[[0.4429]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y_hat[1,3,1])\n",
    "\n",
    "\n",
    "outReshaped = y_hat.view(4,5,2,2)# batchsize,#seqlen,#directions,#hidden\n",
    "y_hat_fwd = outReshaped[:,:,0,:]\n",
    "y_hat_bck = outReshaped[:,:,1,:]\n",
    "print(y_hat_fwd.shape)\n",
    "\n",
    "print(h.shape)\n",
    "\n",
    "\n",
    "hReshaped = h.view(3,2,4,2)#no layers,directions,batchsize, hidden\n",
    "h_fwd = hReshaped[:,0,:,:]\n",
    "h_bck = hReshaped[:,1,:,:]\n",
    "\n",
    "print(h_fwd.shape)\n",
    "\n",
    "y_hat,h = rnn_lstm(inputs)\n",
    "\n",
    "print(y_hat.shape)\n",
    "\n",
    "\n",
    "print(h[0].shape,h[1].shape)\n",
    "\n",
    "y_hat,h = rnn(inputs)\n",
    "\n",
    "print(y_hat.shape)\n",
    "\n",
    "print(y_hat[1,3,1])\n",
    "\n",
    "print(h.shape)\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):# input_size = featuresize, hidden_size = No nurions\n",
    "        super(RNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.E = nn.Embedding(input_size,hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size,hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size,output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,input,hidden):\n",
    "        emb = self.E(input).view(1,1,-1)\n",
    "        output,hidden = self.rnn(emb,hidden)\n",
    "        outLinear = self.linear(output)\n",
    "        outSig = self.sigmoid(outLinear)\n",
    "        return outSig,hidden\n",
    "\n",
    "hidden_size = 128\n",
    "model = RNN(20,hidden_size,1)\n",
    "\n",
    "\n",
    "h = torch.zeros(1,1,hidden_size)\n",
    "inputs = torch.LongTensor([2,3,4,8])\n",
    "for input in inputs:\n",
    "    y,h = model(input,h)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
