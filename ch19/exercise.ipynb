{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 19\n",
    "\n",
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7112152576446533\n",
      "0.7112152576446533\n",
      "0.7112151384353638\n",
      "0.7112150192260742\n",
      "0.7112149000167847\n",
      "0.7112147808074951\n",
      "0.7112147808074951\n",
      "0.7112146615982056\n",
      "0.711214542388916\n",
      "0.7112144231796265\n",
      "0.7112143039703369\n",
      "0.7112141847610474\n",
      "0.7112141847610474\n",
      "0.7112140655517578\n",
      "0.7112139463424683\n",
      "0.7112138271331787\n",
      "0.7112138271331787\n",
      "0.7112137079238892\n",
      "0.7112135887145996\n",
      "0.7112134695053101\n",
      "0.7112132906913757\n",
      "0.7112132906913757\n",
      "0.7112131714820862\n",
      "0.7112130522727966\n",
      "0.7112129330635071\n",
      "0.7112128138542175\n",
      "0.7112128138542175\n",
      "0.7112125754356384\n",
      "0.7112124562263489\n",
      "0.7112124562263489\n",
      "0.7112123370170593\n",
      "0.7112122178077698\n",
      "0.7112120985984802\n",
      "0.7112119793891907\n",
      "0.7112119793891907\n",
      "0.7112118601799011\n",
      "0.7112117409706116\n",
      "0.711211621761322\n",
      "0.7112115025520325\n",
      "0.7112115025520325\n",
      "0.7112113833427429\n",
      "0.7112112641334534\n",
      "0.7112111449241638\n",
      "0.7112111449241638\n",
      "0.7112109065055847\n",
      "0.7112109065055847\n",
      "0.7112107872962952\n",
      "0.7112106680870056\n",
      "0.7112105488777161\n",
      "0.7112105488777161\n",
      "0.7112104296684265\n",
      "0.711210310459137\n",
      "0.7112101912498474\n",
      "0.7112100124359131\n",
      "0.7112100124359131\n",
      "0.7112098932266235\n",
      "0.711209774017334\n",
      "0.7112096548080444\n",
      "0.7112095355987549\n",
      "0.7112095355987549\n",
      "0.7112094163894653\n",
      "0.7112092971801758\n",
      "0.7112091779708862\n",
      "0.7112090587615967\n",
      "0.7112089395523071\n",
      "0.7112088203430176\n",
      "0.711208701133728\n",
      "0.711208701133728\n",
      "0.7112085819244385\n",
      "0.7112084627151489\n",
      "0.7112083435058594\n",
      "0.7112082242965698\n",
      "0.7112082242965698\n",
      "0.7112081050872803\n",
      "0.7112079858779907\n",
      "0.7112078666687012\n",
      "0.7112078666687012\n",
      "0.7112077474594116\n",
      "0.7112076282501221\n",
      "0.7112075090408325\n",
      "0.711207389831543\n",
      "0.7112072706222534\n",
      "0.7112072706222534\n",
      "0.7112071514129639\n",
      "0.7112070322036743\n",
      "0.7112069129943848\n",
      "0.7112067937850952\n",
      "0.7112067937850952\n",
      "0.7112066149711609\n",
      "0.7112064957618713\n",
      "0.7112063765525818\n",
      "0.7112063765525818\n",
      "0.7112062573432922\n",
      "0.7112061381340027\n",
      "0.7112060189247131\n",
      "0.7112058997154236\n",
      "0.7112058997154236\n",
      "0.7112056612968445\n",
      "0.7112055420875549\n",
      "0.7112054228782654\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "loss_fun = nn.BCELoss()\n",
    "lr = 0.0001\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float)\n",
    "w = torch.randn(1,requires_grad=True)\n",
    "\n",
    "nIter = 100\n",
    "for i in range(nIter):\n",
    "    y_hat = m(w*x)\n",
    "    loss = loss_fun(y_hat,y)\n",
    "    loss.backward()\n",
    "    dw = w.grad.data\n",
    "    with torch.no_grad():\n",
    "        w -= lr*dw\n",
    "    w.grad.data.zero_()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(1000,)\n",
      "Loss after epoch=0: 0.014824\n",
      "Loss after epoch=1: 0.014810\n",
      "Loss after epoch=2: 0.014795\n",
      "Loss after epoch=3: 0.014781\n",
      "Loss after epoch=4: 0.014766\n",
      "Loss after epoch=5: 0.014752\n",
      "Loss after epoch=6: 0.014738\n",
      "Loss after epoch=7: 0.014723\n",
      "Loss after epoch=8: 0.014709\n",
      "Loss after epoch=9: 0.014695\n",
      "Loss after epoch=10: 0.014681\n",
      "Loss after epoch=11: 0.014667\n",
      "Loss after epoch=12: 0.014653\n",
      "Loss after epoch=13: 0.014639\n",
      "Loss after epoch=14: 0.014625\n",
      "Loss after epoch=15: 0.014611\n",
      "Loss after epoch=16: 0.014597\n",
      "Loss after epoch=17: 0.014584\n",
      "Loss after epoch=18: 0.014570\n",
      "Loss after epoch=19: 0.014556\n",
      "Loss after epoch=20: 0.014543\n",
      "Loss after epoch=21: 0.014529\n",
      "Loss after epoch=22: 0.014516\n",
      "Loss after epoch=23: 0.014502\n",
      "Loss after epoch=24: 0.014489\n",
      "Loss after epoch=25: 0.014476\n",
      "Loss after epoch=26: 0.014462\n",
      "Loss after epoch=27: 0.014449\n",
      "Loss after epoch=28: 0.014436\n",
      "Loss after epoch=29: 0.014423\n",
      "Loss after epoch=30: 0.014410\n",
      "Loss after epoch=31: 0.014397\n",
      "Loss after epoch=32: 0.014384\n",
      "Loss after epoch=33: 0.014371\n",
      "Loss after epoch=34: 0.014358\n",
      "Loss after epoch=35: 0.014345\n",
      "Loss after epoch=36: 0.014333\n",
      "Loss after epoch=37: 0.014320\n",
      "Loss after epoch=38: 0.014307\n",
      "Loss after epoch=39: 0.014295\n",
      "Loss after epoch=40: 0.014282\n",
      "Loss after epoch=41: 0.014269\n",
      "Loss after epoch=42: 0.014257\n",
      "Loss after epoch=43: 0.014245\n",
      "Loss after epoch=44: 0.014232\n",
      "Loss after epoch=45: 0.014220\n",
      "Loss after epoch=46: 0.014208\n",
      "Loss after epoch=47: 0.014196\n",
      "Loss after epoch=48: 0.014183\n",
      "Loss after epoch=49: 0.014171\n",
      "Loss after epoch=50: 0.014159\n",
      "Loss after epoch=51: 0.014147\n",
      "Loss after epoch=52: 0.014135\n",
      "Loss after epoch=53: 0.014123\n",
      "Loss after epoch=54: 0.014111\n",
      "Loss after epoch=55: 0.014100\n",
      "Loss after epoch=56: 0.014088\n",
      "Loss after epoch=57: 0.014076\n",
      "Loss after epoch=58: 0.014064\n",
      "Loss after epoch=59: 0.014053\n",
      "Loss after epoch=60: 0.014041\n",
      "Loss after epoch=61: 0.014030\n",
      "Loss after epoch=62: 0.014018\n",
      "Loss after epoch=63: 0.014007\n",
      "Loss after epoch=64: 0.013995\n",
      "Loss after epoch=65: 0.013984\n",
      "Loss after epoch=66: 0.013972\n",
      "Loss after epoch=67: 0.013961\n",
      "Loss after epoch=68: 0.013950\n",
      "Loss after epoch=69: 0.013939\n",
      "Loss after epoch=70: 0.013928\n",
      "Loss after epoch=71: 0.013917\n",
      "Loss after epoch=72: 0.013905\n",
      "Loss after epoch=73: 0.013894\n",
      "Loss after epoch=74: 0.013884\n",
      "Loss after epoch=75: 0.013873\n",
      "Loss after epoch=76: 0.013862\n",
      "Loss after epoch=77: 0.013851\n",
      "Loss after epoch=78: 0.013840\n",
      "Loss after epoch=79: 0.013829\n",
      "Loss after epoch=80: 0.013819\n",
      "Loss after epoch=81: 0.013808\n",
      "Loss after epoch=82: 0.013797\n",
      "Loss after epoch=83: 0.013787\n",
      "Loss after epoch=84: 0.013776\n",
      "Loss after epoch=85: 0.013766\n",
      "Loss after epoch=86: 0.013755\n",
      "Loss after epoch=87: 0.013745\n",
      "Loss after epoch=88: 0.013734\n",
      "Loss after epoch=89: 0.013724\n",
      "Loss after epoch=90: 0.013714\n",
      "Loss after epoch=91: 0.013704\n",
      "Loss after epoch=92: 0.013693\n",
      "Loss after epoch=93: 0.013683\n",
      "Loss after epoch=94: 0.013673\n",
      "Loss after epoch=95: 0.013663\n",
      "Loss after epoch=96: 0.013653\n",
      "Loss after epoch=97: 0.013643\n",
      "Loss after epoch=98: 0.013633\n",
      "Loss after epoch=99: 0.013623\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "\n",
    "def wSum(X,W):\n",
    "    h = torch.from_numpy(X)\n",
    "    z = torch.matmul(W,h)\n",
    "    return z\n",
    "\n",
    "def forwardStep(X,W_list):\n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h)\n",
    "        h = activate(z)\n",
    "    return h\n",
    "\n",
    "def activate(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def updateParams(W_list,dW_list,lr):\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(W_list)):\n",
    "            W_list[i] -= lr*dW_list[i]\n",
    "    return W_list\n",
    "\n",
    "def trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001,nepochs=100):\n",
    "    for epoch in range(nepochs):\n",
    "        avgLoss = []\n",
    "        for i in range(len(y)):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss = loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss.backward()\n",
    "            avgLoss.append(loss.item())\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,np.mean(np.array(avgLoss))))\n",
    "    return W_list\n",
    "\n",
    "def trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100):\n",
    "    n = len(y)\n",
    "    for epoch in range(nepochs):\n",
    "        loss = 0\n",
    "        for i in range(n):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "        loss = loss/n\n",
    "        loss.backward()\n",
    "        sys.stdout.flush()\n",
    "        dW_list = []\n",
    "        for j in range(len(W_list)):\n",
    "            dW_list.append(W_list[j].grad.data)\n",
    "        W_list = updateParams(W_list,dW_list,lr)\n",
    "        for j in range(len(W_list)):\n",
    "            W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "    return W_list\n",
    "\n",
    "def trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100,batchSize=16):\n",
    "    n = len(y)\n",
    "    numBatches = n//batchSize\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch in range(numBatches):\n",
    "            X_batch = X[batch*batchSize:(batch+1)*batchSize,:]\n",
    "            y_batch = y[batch*batchSize:(batch+1)*batchSize]\n",
    "            loss = 0\n",
    "            for i in range(batchSize):\n",
    "                Xin = X_batch[i,:]\n",
    "                yTrue = y_batch[i]\n",
    "                y_hat = forwardStep(Xin,W_list)\n",
    "                loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss = loss/batchSize\n",
    "            loss.backward()\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss/numBatches))\n",
    "    return W_list\n",
    "\n",
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "np.unique(y)\n",
    "\n",
    "W = torch.tensor(np.random.uniform(0,1,inputDim),requires_grad=True)\n",
    "\n",
    "z = wSum(X[0,:],W)\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True) #(2,inputDim) = (no. neurons, no imputs)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "#W_list = trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)\n",
    "#W_list = trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)\n",
    "W_list = trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "tensor_x = torch.Tensor(X)\n",
    "tensor_y = torch.Tensor(y)\n",
    "Xy = TensorDataset(tensor_x,tensor_y)\n",
    "Xy_loader = DataLoader(Xy,batch_size=16,shuffle=True,drop_last=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(inputDim,200),\n",
    "    nn.ReLU(),\n",
    "    #nn.BatchNorm1d(num_features=200),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(200,100),\n",
    "    nn.Tanh(),\n",
    "    #nn.BatchNorm1d(num_features=100),\n",
    "    nn.Linear(100,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "nepochs = 100\n",
    "for epoch in range(nepochs):\n",
    "    for X,y in Xy_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(float(loss))\n",
    "\n",
    "with torch.no_grad():\n",
    "    xt = torch.tensor(np.random.rand(1,inputDim))\n",
    "    y2 = model(xt.float())\n",
    "    print(y2.detach().numpy()[0][0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
